import re
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Any

import pandas as pd
import requests
import streamlit as st

from bs4 import BeautifulSoup
import tldextract
import plotly.express as px


# -----------------------------
# Page Config
# -----------------------------
st.set_page_config(
    page_title="ì„¹ì…˜ë³„ Top 5 + ì„±í–¥ ë¶„í¬ (ì „ë‚  ê¸°ì¤€ / Debug ê°•í™”)",
    page_icon="ğŸ—ï¸",
    layout="wide",
)

st.markdown(
    """
<style>
.block-container { padding-top: 1.1rem; padding-bottom: 2rem; }
small.muted { color: rgba(49,51,63,.65); }
.card {
  border: 1px solid rgba(49,51,63,.15);
  border-radius: 14px;
  padding: 14px 16px;
  margin-bottom: 10px;
  background: rgba(255,255,255,.02);
}
.card h4 { margin: 0 0 8px 0; }
.kv { display: flex; gap: 12px; flex-wrap: wrap; margin: 6px 0 10px 0; }
.kv span { font-size: 0.92rem; color: rgba(49,51,63,.72); }
.badge { display:inline-block; padding:2px 8px; border-radius: 999px; border:1px solid rgba(49,51,63,.18); font-size:.82rem;}
hr.soft { border: none; border-top: 1px solid rgba(49,51,63,.10); margin: 14px 0; }
ul.tight { margin: 0.2rem 0 0.2rem 1.2rem; }
code.small { font-size: 0.85rem; }
</style>
""",
    unsafe_allow_html=True,
)

# -----------------------------
# Constants
# -----------------------------
KST = timezone(timedelta(hours=9))
UTC = timezone.utc

GDELT_DOC_ENDPOINT = "https://api.gdeltproject.org/api/v2/doc/doc"
USER_AGENT = "Mozilla/5.0 (compatible; StreamlitSectionTop5/2.5; +https://streamlit.io)"
REQUEST_TIMEOUT = 15  # seconds

BIAS_ORDER = ["ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´", "ë¯¸ë¶„ë¥˜"]
EMPTY_COLUMNS = ["title", "url", "seendate", "published_utc", "sourceCountry", "language", "domain"]


# -----------------------------
# Models / Config
# -----------------------------
@dataclass(frozen=True)
class SectionQuery:
    section: str
    domestic_query: str
    overseas_query: str


# NOTE: DOC ê²€ìƒ‰ ì•ˆì •ì„±ì„ ìœ„í•´ êµ­ë‚´/í•´ì™¸ ëª¨ë‘ ì˜ì–´ í‚¤ì›Œë“œ ê¸°ë°˜
SECTIONS: List[SectionQuery] = [
    SectionQuery(
        section="ì •ì¹˜",
        domestic_query="(politics OR government OR parliament OR national assembly OR president OR election OR ruling party OR opposition)",
        overseas_query="(politics OR government OR parliament OR congress OR president OR election OR campaign)",
    ),
    SectionQuery(
        section="ê²½ì œ",
        domestic_query="(economy OR markets OR stocks OR exchange rate OR interest rates OR inflation OR prices OR companies OR industry OR semiconductor)",
        overseas_query="(economy OR markets OR stocks OR inflation OR interest rates OR central bank OR currency OR business OR industry OR semiconductor)",
    ),
    SectionQuery(
        section="ì‚¬íšŒ",
        domestic_query="(crime OR accident OR disaster OR education OR health OR welfare OR labor OR strike OR court OR prosecutors OR police)",
        overseas_query="(society OR crime OR accident OR disaster OR education OR health OR welfare OR labor OR strike OR court OR police)",
    ),
    SectionQuery(
        section="êµ­ì œ",
        domestic_query="(diplomacy OR summit OR UN OR United Nations OR United States OR China OR Japan OR Russia OR Ukraine OR Middle East OR Gaza)",
        overseas_query="(world OR international OR diplomacy OR summit OR UN OR Ukraine OR Russia OR China OR Japan OR Middle East OR Gaza)",
    ),
    SectionQuery(
        section="ìŠ¤í¬ì¸ ",
        domestic_query="(sports OR soccer OR football OR baseball OR basketball OR volleyball OR golf OR esports OR Olympics OR World Cup)",
        overseas_query="(sports OR football OR soccer OR baseball OR basketball OR Olympics OR World Cup OR NBA OR MLB OR NHL)",
    ),
]


# -----------------------------
# Bias mapping
# -----------------------------
def default_bias_mapping_df() -> pd.DataFrame:
    data = [
        ("chosun.com", "ë³´ìˆ˜"),
        ("donga.com", "ë³´ìˆ˜"),
        ("joongang.co.kr", "ì¤‘ë„"),
        ("mk.co.kr", "ì¤‘ë„"),
        ("yonhapnews.co.kr", "ì¤‘ë„"),
        ("hani.co.kr", "ì§„ë³´"),
        ("khan.co.kr", "ì§„ë³´"),
        ("reuters.com", "ì¤‘ë„"),
        ("apnews.com", "ì¤‘ë„"),
        ("bbc.co.uk", "ì¤‘ë„"),
        ("economist.com", "ì¤‘ë„"),
        ("foxnews.com", "ë³´ìˆ˜"),
        ("wsj.com", "ë³´ìˆ˜"),
        ("nytimes.com", "ì§„ë³´"),
        ("washingtonpost.com", "ì§„ë³´"),
        ("cnn.com", "ì§„ë³´"),
    ]
    return pd.DataFrame(data, columns=["domain", "bias"])


# -----------------------------
# Helpers
# -----------------------------
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()


def escape_html(s: str) -> str:
    s = s or ""
    return (
        s.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#39;")
    )


def normalize_domain(url: str) -> Optional[str]:
    if not url or not isinstance(url, str):
        return None
    try:
        ext = tldextract.extract(url)
        return ext.registered_domain.lower() if ext.registered_domain else None
    except Exception:
        return None


def parse_seendate_utc(s: str) -> Optional[datetime]:
    if not s or not isinstance(s, str):
        return None
    try:
        return datetime.strptime(s, "%Y%m%d%H%M%S").replace(tzinfo=UTC)
    except Exception:
        return None


def yesterday_kst_range_utc() -> Tuple[datetime, datetime]:
    """
    ì „ë‚  00:00 ~ ì˜¤ëŠ˜ 00:00 (KST) ë²”ìœ„ë¥¼ UTCë¡œ ë³€í™˜
    """
    now_kst = datetime.now(KST)
    today_start_kst = now_kst.replace(hour=0, minute=0, second=0, microsecond=0)
    start_kst = today_start_kst - timedelta(days=1)
    end_kst = today_start_kst
    return start_kst.astimezone(UTC), end_kst.astimezone(UTC)


# -----------------------------
# Query builder: candidates with fallback
# -----------------------------
def build_section_query_candidates(region: str, section_cfg: SectionQuery, extra_keyword: str) -> List[str]:
    extra = clean_text(extra_keyword)

    # extra í¬í•¨ -> (0ì´ë©´) extra ì œê±°
    extra_parts = []
    if extra:
        extra_parts.append(f'("{extra}")')
    extra_parts.append("")

    queries: List[str] = []

    if region == "êµ­ë‚´":
        lang_candidates = ["sourcelang:kor", "sourcelang:korean", "sourcelang:Korean"]
        for lang in lang_candidates:
            for extra_part in extra_parts:
                queries.append(f"{lang} {section_cfg.domestic_query} {extra_part}".strip())
        # ìµœí›„ í´ë°±: ì–¸ì–´ ì œí•œ ì œê±°
        for extra_part in extra_parts:
            queries.append(f"{section_cfg.domestic_query} {extra_part}".strip())
        return queries

    # í•´ì™¸
    lang_candidates = ["sourcelang:eng", "sourcelang:english", "sourcelang:English"]
    for lang in lang_candidates:
        for extra_part in extra_parts:
            queries.append(f"{lang} {section_cfg.overseas_query} {extra_part}".strip())
    for extra_part in extra_parts:
        queries.append(f"{section_cfg.overseas_query} {extra_part}".strip())
    return queries


# -----------------------------
# GDELT fetch (Debug ê°•í™”)
# -----------------------------
@st.cache_data(ttl=60 * 10, show_spinner=False)
def fetch_gdelt_articles(
    query: str,
    start_dt_utc: datetime,
    end_dt_utc: datetime,
    max_records: int,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Returns (df, debug_info)
    debug_info includes status_code, final_url, top-level keys, error/message (if present), articles_count
    """
    def fmt(dt: datetime) -> str:
        return dt.astimezone(UTC).strftime("%Y%m%d%H%M%S")

    params = {
        "query": query,
        # IMPORTANT: modeëŠ” ì˜ˆì‹œëŒ€ë¡œ ì†Œë¬¸ì ì‚¬ìš©
        "mode": "artlist",
        "format": "json",
        "maxrecords": int(max_records),
        # ë‚ ì§œ í™•ì¸ í¸ì˜: ìµœì‹ ìˆœ
        "sort": "datedesc",
        "startdatetime": fmt(start_dt_utc),
        "enddatetime": fmt(end_dt_utc),
    }

    headers = {"User-Agent": USER_AGENT}

    debug_info: Dict[str, Any] = {
        "status_code": None,
        "final_url": None,
        "top_keys": None,
        "error": None,
        "message": None,
        "articles_count": None,
    }

    r = requests.get(GDELT_DOC_ENDPOINT, params=params, headers=headers, timeout=REQUEST_TIMEOUT)
    debug_info["status_code"] = r.status_code
    debug_info["final_url"] = r.url

    r.raise_for_status()
    data = r.json()

    if isinstance(data, dict):
        debug_info["top_keys"] = sorted(list(data.keys()))
        debug_info["error"] = data.get("error")
        debug_info["message"] = data.get("message")
        debug_info["articles_count"] = len(data.get("articles", []) or [])
    else:
        debug_info["top_keys"] = [type(data).__name__]

    rows = []
    for a in (data.get("articles", []) or []):
        url = a.get("url")
        rows.append(
            {
                "title": clean_text(a.get("title") or ""),
                "url": url,
                "seendate": a.get("seendate"),
                "published_utc": parse_seendate_utc(a.get("seendate")),
                "sourceCountry": a.get("sourceCountry"),
                "language": a.get("language"),
                "domain": normalize_domain(url) or "unknown",
            }
        )

    if not rows:
        return pd.DataFrame(columns=EMPTY_COLUMNS), debug_info

    df = pd.DataFrame(rows)
    df["published_utc"] = pd.to_datetime(df["published_utc"], utc=True, errors="coerce")
    df = df.dropna(subset=["published_utc"])
    df = df[df["title"] != ""]
    df = df.drop_duplicates(subset=["url"], keep="first")
    return df, debug_info


# -----------------------------
# Summarization (same as before)
# -----------------------------
@st.cache_data(ttl=60 * 60, show_spinner=False)
def fetch_page_text_and_meta(url: str) -> Tuple[str, str]:
    if not url:
        return "", ""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }
    try:
        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        if r.status_code >= 400:
            return "", ""
        soup = BeautifulSoup(r.text, "lxml")

        desc = ""
        og = soup.find("meta", property="og:description")
        if og and og.get("content"):
            desc = clean_text(og.get("content"))

        if not desc:
            meta = soup.find("meta", attrs={"name": "description"})
            if meta and meta.get("content"):
                desc = clean_text(meta.get("content"))

        paras = soup.find_all("p")
        texts = []
        for p in paras[:10]:
            t = clean_text(p.get_text(" ", strip=True))
            if len(t) >= 40:
                texts.append(t)

        body = " ".join(texts)[:2000]
        return body, desc
    except Exception:
        return "", ""


def split_sentences(text: str) -> List[str]:
    text = clean_text(text)
    if not text:
        return []
    parts = re.split(r"(?<=[\.\!\?])\s+|(?<=\n)\s*", text)
    out: List[str] = []
    for p in parts:
        p = clean_text(p)
        if 25 <= len(p) <= 220:
            out.append(p)

    seen = set()
    uniq = []
    for s in out:
        key = re.sub(r"[^0-9A-Za-zê°€-í£]+", "", s).lower()
        if key and key not in seen:
            seen.add(key)
            uniq.append(s)
    return uniq


def summarize_3_bullets(page_text: str, meta_desc: str) -> List[str]:
    sents = split_sentences(page_text)
    bullets: List[str] = []

    for s in sents:
        if len(bullets) >= 3:
            break
        low = s.lower()
        if any(k in low for k in ["cookies", "subscribe", "sign up", "ê´‘ê³ ", "ì €ì‘ê¶Œ", "ë¬´ë‹¨", "êµ¬ë…"]):
            continue
        bullets.append(s)

    if len(bullets) < 3 and meta_desc:
        md = clean_text(meta_desc)
        chunks = re.split(r"[â€¢\-\|/]\s*", md)
        for c in chunks:
            c = clean_text(c)
            if 25 <= len(c) <= 220 and c not in bullets:
                bullets.append(c)
            if len(bullets) >= 3:
                break

    return bullets[:3]


# -----------------------------
# Dedup clustering (SAFE)
# -----------------------------
STOPWORDS_KO = set(
    "ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ë˜í•œ ë•Œë¬¸ì— í†µí•´ ê´€ë ¨ ëŒ€í•œ ë”°ë¥´ë©´ ê²½ìš° ì´ë²ˆ ì˜¤ëŠ˜ ë‚´ì¼ ì–´ì œ ê¸°ì ë‹¨ë… ì†ë³´ "
    "ì˜ìƒ ì‚¬ì§„ ë°œí‘œ ë°í˜”ë‹¤ ë§í–ˆë‹¤ ì˜ˆì • ì§„í–‰ ê°€ëŠ¥ í™•ëŒ€ ê°ì†Œ ì¦ê°€ ì •ë¶€ êµ­íšŒ ëŒ€í†µë ¹ ".split()
)
STOPWORDS_EN = set(
    "the a an and or but if then than this that those these to of in on for with without "
    "as from by at is are was were be been being it its into about after before over under "
    "says said say will would could should may might ".split()
)


def title_tokens(title: str) -> List[str]:
    t = clean_text(title).lower()
    t = re.sub(r"https?://\S+", " ", t)
    t = re.sub(r"[^0-9a-zê°€-í£\s]", " ", t)
    toks = [x for x in t.split() if len(x) >= 2]
    filtered: List[str] = []
    for x in toks:
        if re.fullmatch(r"\d+", x):
            filtered.append(x)
            continue
        if x in STOPWORDS_EN or x in STOPWORDS_KO:
            continue
        filtered.append(x)
    return filtered[:32]


def jaccard(a: set, b: set) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0


def dedup_by_title_cluster(df: pd.DataFrame, sim_threshold: float = 0.62) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if "title" not in df.columns or "published_utc" not in df.columns:
        return df

    dfx = df.copy().sort_values("published_utc", ascending=False)
    kept_idx: List[int] = []
    cluster_reps: List[set] = []

    for idx, row in dfx.iterrows():
        toks = set(title_tokens(row.get("title", "")))
        if not toks:
            continue

        dup = False
        for rep in cluster_reps:
            if jaccard(toks, rep) >= sim_threshold:
                dup = True
                break

        if not dup:
            kept_idx.append(idx)
            cluster_reps.append(toks)

    if not kept_idx:
        return dfx.head(0)

    return dfx.loc[kept_idx].copy()


# -----------------------------
# Bias mapping + distribution
# -----------------------------
def apply_bias_mapping(df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    out = df.copy()
    out["bias"] = out["domain"].map(lambda d: mapping.get((d or "").lower(), "ë¯¸ë¶„ë¥˜"))
    out["bias"] = out["bias"].where(out["bias"].isin(["ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´"]), "ë¯¸ë¶„ë¥˜")
    return out


def distribution(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty or "bias" not in df.columns:
        return pd.DataFrame(columns=["bias", "count", "share"])
    dist = df.groupby("bias").size().reset_index(name="count")
    total = dist["count"].sum()
    dist["share"] = dist["count"] / total if total else 0
    dist["bias"] = pd.Categorical(dist["bias"], categories=BIAS_ORDER, ordered=True)
    return dist.sort_values("bias")


# -----------------------------
# Rendering
# -----------------------------
def render_top_list(section_name: str, top_df: pd.DataFrame, enable_summary: bool):
    st.subheader(f"{section_name} Â· Top {len(top_df)}")
    if top_df is None or top_df.empty:
        st.warning("í•´ë‹¹ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    for idx, row in top_df.reset_index(drop=True).iterrows():
        title = row.get("title") or "(ì œëª© ì—†ìŒ)"
        url = row.get("url") or ""
        domain = row.get("domain") or "unknown"
        bias = row.get("bias") or "ë¯¸ë¶„ë¥˜"

        pub_str = ""
        try:
            pub_kst = pd.to_datetime(row.get("published_utc"), utc=True).tz_convert(KST)
            pub_str = pub_kst.strftime("%Y-%m-%d %H:%M (KST)")
        except Exception:
            pass

        bullets: List[str] = row.get("bullets") or []
        meta_desc = row.get("meta_desc") or ""

        if enable_summary:
            if bullets:
                summary_html = "<ul class='tight'>" + "".join([f"<li>{escape_html(b)}</li>" for b in bullets]) + "</ul>"
            elif meta_desc:
                summary_html = f"<small class='muted'>{escape_html(meta_desc)}</small>"
            else:
                summary_html = "<small class='muted'>ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ì°¨ë‹¨/ë³¸ë¬¸ ë¶€ì¬ ê°€ëŠ¥).</small>"
        else:
            summary_html = "<small class='muted'>ìš”ì•½ ê¸°ëŠ¥ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤.</small>"

        st.markdown(
            f"""
<div class="card">
  <div class="kv">
    <span class="badge">#{idx+1}</span>
    <span>ì„±í–¥: <b>{escape_html(bias)}</b></span>
    <span>ë„ë©”ì¸: <b>{escape_html(domain)}</b></span>
    <span>ë°œí–‰: <b>{escape_html(pub_str)}</b></span>
  </div>
  <h4>{escape_html(title)}</h4>
  <div>
    <a href="{url}" target="_blank" rel="noopener noreferrer">{url}</a>
  </div>
  <hr class="soft"/>
  <div>
    <b>í•µì‹¬ ìš”ì•½ (3 bullets)</b><br/>
    {summary_html}
  </div>
</div>
""",
            unsafe_allow_html=True,
        )


# -----------------------------
# UI
# -----------------------------
st.title("ì„¹ì…˜ë³„ ì£¼ìš” ë‰´ìŠ¤ Top 5 + ì„±í–¥ ë¶„í¬ (ì „ë‚  ê¸°ì¤€)")
st.caption("ì „ë‚  00:00~24:00(KST) ê¸°ì¤€. í›„ë³´ 0ê±´ ì›ì¸ ì§„ë‹¨ì„ ìœ„í•´ GDELT ì‘ë‹µ ë””ë²„ê·¸ë¥¼ ê°•í™”í–ˆìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.header("1) ë²”ìœ„ ì„ íƒ")
    region = st.radio("êµ­ë‚´/í•´ì™¸", options=["êµ­ë‚´", "í•´ì™¸"], horizontal=True)

    st.divider()
    st.header("2) ì„¹ì…˜ ì„ íƒ")
    section_names = [s.section for s in SECTIONS]
    selected_sections = st.multiselect(
        "ë¶„ì„í•  ì„¹ì…˜(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
        options=section_names,
        default=section_names,
    )

    st.divider()
    st.header("3) Top ë‰´ìŠ¤ êµ¬ì„±")
    extra_keyword = st.text_input("ì¶”ê°€ í‚¤ì›Œë“œ(ì„ íƒ)", value="")

    top_n = st.number_input("ì„¹ì…˜ë³„ Top N", min_value=3, max_value=10, value=5, step=1)
    candidate_pool = st.number_input("ì„¹ì…˜ë³„ í›„ë³´ ê¸°ì‚¬ ìˆ˜(ìˆ˜ì§‘ëŸ‰)", min_value=60, max_value=500, value=250, step=10)

    st.divider()
    st.header("í’ˆì§ˆ ì˜µì…˜")
    enable_summary = st.toggle("3ì¤„ í•µì‹¬ bullet ìš”ì•½", value=True)
    sim_threshold = st.slider("ì¤‘ë³µ ì œê±° ìœ ì‚¬ë„ ì„ê³„ê°’(Jaccard)", 0.45, 0.80, 0.62, 0.01)

    st.divider()
    st.header("ì„±í–¥ ë§¤í•‘")
    uploaded = st.file_uploader("ë§¤í•‘ CSV ì—…ë¡œë“œ (domain,bias)", type=["csv"])
    if uploaded is not None:
        try:
            map_df = pd.read_csv(uploaded)[["domain", "bias"]].dropna()
        except Exception:
            st.warning("CSVë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. columns: domain,bias í˜•íƒœì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            map_df = default_bias_mapping_df()
    else:
        map_df = default_bias_mapping_df()

    edited_map_df = st.data_editor(
        map_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "domain": st.column_config.TextColumn("domain"),
            "bias": st.column_config.SelectboxColumn("bias", options=["ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´"]),
        },
    )

    mapping_dict = {
        str(r["domain"]).strip().lower(): str(r["bias"]).strip()
        for _, r in edited_map_df.dropna().iterrows()
        if str(r.get("domain", "")).strip() and str(r.get("bias", "")).strip()
    }

    st.divider()
    debug = st.toggle("ë””ë²„ê·¸ í‘œì‹œ(ìš”ì²­ URL/ì‘ë‹µ í‚¤/ê±´ìˆ˜)", value=True)
    run = st.button("ì „ë‚  ì„¹ì…˜ë³„ Top ë‰´ìŠ¤ ìƒì„±", type="primary", use_container_width=True)

if not run:
    st.info("ì¢Œì¸¡ì—ì„œ ì„ íƒ í›„ ì‹¤í–‰í•˜ì„¸ìš”.")
    st.stop()

if not selected_sections:
    st.warning("ìµœì†Œ 1ê°œ ì„¹ì…˜ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    st.stop()

start_utc, end_utc = yesterday_kst_range_utc()
start_kst = start_utc.astimezone(KST)
end_kst = end_utc.astimezone(KST)

st.markdown(f"### {region} Â· ì„¹ì…˜ë³„ Top {int(top_n)}")
st.caption(f"ìˆ˜ì§‘ ê¸°ê°„: {start_kst.strftime('%Y-%m-%d %H:%M')} ~ {end_kst.strftime('%Y-%m-%d %H:%M')} (KST)")


# -----------------------------
# Connectivity self-test (3ë‹¨ê³„)
# -----------------------------
with st.expander("ì§„ë‹¨: GDELT ì—°ê²° í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„)", expanded=True):
    tests = [
        ('"Korea"', 'ì–¸ì–´ í•„í„° ì—†ìŒ'),
        ('sourcelang:eng "Korea"', 'ì˜ì–´ ì†ŒìŠ¤ í•„í„°'),
        ('domain:cnn.com "Korea"', 'ë„ë©”ì¸ í•„í„°'),
    ]

    for q, label in tests:
        st.markdown(f"- **{label}**: <code class='small'>{escape_html(q)}</code>", unsafe_allow_html=True)
        try:
            df_t, dbg = fetch_gdelt_articles(
                query=q,
                start_dt_utc=start_utc,
                end_dt_utc=end_utc,
                max_records=5,
            )
            if debug:
                st.write(
                    {
                        "status_code": dbg.get("status_code"),
                        "articles_count": dbg.get("articles_count"),
                        "error": dbg.get("error"),
                        "message": dbg.get("message"),
                        "top_keys": dbg.get("top_keys"),
                    }
                )
                st.write("final_url:", dbg.get("final_url"))

            st.write("rows:", len(df_t))
            if not df_t.empty:
                st.dataframe(df_t[["published_utc", "domain", "title", "url"]], use_container_width=True)
        except Exception as e:
            st.error(f"í˜¸ì¶œ ì‹¤íŒ¨: {repr(e)}")


# -----------------------------
# Main processing
# -----------------------------
section_cfg_map: Dict[str, SectionQuery] = {s.section: s for s in SECTIONS}
results: Dict[str, Dict[str, Any]] = {}

with st.spinner("ì„¹ì…˜ë³„ ê¸°ì‚¬ í›„ë³´ë¥¼ ìˆ˜ì§‘/ì •ì œ ì¤‘ì…ë‹ˆë‹¤..."):
    for sec_name in selected_sections:
        cfg = section_cfg_map[sec_name]
        query_candidates = build_section_query_candidates(region, cfg, extra_keyword)

        df = pd.DataFrame(columns=EMPTY_COLUMNS)
        used_q = query_candidates[0]
        used_dbg: Dict[str, Any] = {}

        for cand_q in query_candidates:
            used_q = cand_q
            try:
                df_try, dbg_try = fetch_gdelt_articles(
                    query=cand_q,
                    start_dt_utc=start_utc,
                    end_dt_utc=end_utc,
                    max_records=int(candidate_pool),
                )
            except Exception:
                df_try = pd.DataFrame(columns=EMPTY_COLUMNS)
                dbg_try = {"status_code": None, "final_url": None, "top_keys": None, "error": None, "message": None, "articles_count": None}

            used_dbg = dbg_try

            if debug:
                st.write(f"[DEBUG] {sec_name} query = {cand_q}")
                st.write(f"[DEBUG] {sec_name} status={dbg_try.get('status_code')} articles={dbg_try.get('articles_count')}")
                st.write(f"[DEBUG] {sec_name} final_url = {dbg_try.get('final_url')}")
                if dbg_try.get("error") or dbg_try.get("message"):
                    st.write(f"[DEBUG] {sec_name} error/message =", {"error": dbg_try.get("error"), "message": dbg_try.get("message")})

            if not df_try.empty:
                df = df_try
                break

        df = apply_bias_mapping(df, mapping_dict)
        df_dedup = dedup_by_title_cluster(df, sim_threshold=float(sim_threshold))

        if df_dedup is None or "published_utc" not in df_dedup.columns:
            df_dedup = df.head(0).copy()

        if not df_dedup.empty:
            df_dedup = df_dedup.sort_values("published_utc", ascending=False)

        top_df = df_dedup.head(int(top_n)).copy()

        if enable_summary and not top_df.empty:
            top_df["bullets"] = None
            top_df["meta_desc"] = ""
            for i in range(len(top_df)):
                url = top_df.iloc[i].get("url")
                time.sleep(0.12)
                page_text, meta_desc = fetch_page_text_and_meta(url)
                bullets = summarize_3_bullets(page_text, meta_desc)
                top_df.iat[i, top_df.columns.get_loc("bullets")] = bullets
                top_df.iat[i, top_df.columns.get_loc("meta_desc")] = meta_desc or ""

        dist_df = distribution(df_dedup)

        results[sec_name] = {
            "candidates": df_dedup,
            "top": top_df,
            "dist": dist_df,
            "query": used_q,
            "dbg": used_dbg,
        }


# -----------------------------
# Render
# -----------------------------
tabs = st.tabs(selected_sections)
for tab, sec_name in zip(tabs, selected_sections):
    with tab:
        used_q = results[sec_name]["query"]
        st.markdown(f"<small class='muted'>ìµœì¢… ì‚¬ìš© ì¿¼ë¦¬: {escape_html(clean_text(used_q))}</small>", unsafe_allow_html=True)

        if debug:
            dbg = results[sec_name].get("dbg", {})
            st.write(
                {
                    "status_code": dbg.get("status_code"),
                    "articles_count": dbg.get("articles_count"),
                    "error": dbg.get("error"),
                    "message": dbg.get("message"),
                    "top_keys": dbg.get("top_keys"),
                }
            )
            st.write("final_url:", dbg.get("final_url"))

        cands = results[sec_name]["candidates"]
        dist_df = results[sec_name]["dist"]

        c1, c2, c3 = st.columns(3)
        c1.metric("í›„ë³´ ê¸°ì‚¬(ì¤‘ë³µ ì œê±° í›„)", f"{len(cands):,}" if cands is not None else "0")
        unknown_share = dist_df.loc[dist_df["bias"] == "ë¯¸ë¶„ë¥˜", "share"].sum() if dist_df is not None and not dist_df.empty else 0
        c2.metric("ë¯¸ë¶„ë¥˜ ë¹„ìœ¨", f"{unknown_share*100:.1f}%")
        c3.metric("ê³ ìœ  ë„ë©”ì¸", f"{cands['domain'].nunique(dropna=True):,}" if cands is not None and not cands.empty and "domain" in cands.columns else "0")

        if dist_df is not None and not dist_df.empty:
            fig = px.bar(dist_df, x="bias", y="count", text=dist_df["share"].map(lambda x: f"{x*100:.1f}%"))
            fig.update_layout(xaxis_title="ì„±í–¥", yaxis_title="ê¸°ì‚¬ ìˆ˜", showlegend=False, height=320)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ì„±í–¥ ë¶„í¬ë¥¼ ë§Œë“¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        st.divider()
        render_top_list(sec_name, results[sec_name]["top"], enable_summary)

        with st.expander("ì§„ë‹¨: í›„ë³´ ê¸°ì‚¬(ì¤‘ë³µ ì œê±° í›„) ë¯¸ë¦¬ë³´ê¸°", expanded=False):
            if cands is None or cands.empty:
                st.write("í›„ë³´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                cols = [c for c in ["published_utc", "bias", "domain", "title", "url", "language", "sourceCountry"] if c in cands.columns]
                st.dataframe(cands[cols].head(60), use_container_width=True, height=420)

st.caption(
    "í›„ë³´/í…ŒìŠ¤íŠ¸ê°€ ê³„ì† 0ì´ë©´, ìƒë‹¨ â€˜GDELT ì—°ê²° í…ŒìŠ¤íŠ¸â€™ì˜ status_code, final_url, top_keys, error/messageë¥¼ ê¸°ì¤€ìœ¼ë¡œ "
    "í™˜ê²½ ë¬¸ì œ(DNS/TLS/outbound)ì¸ì§€, íŒŒë¼ë¯¸í„°/ì‘ë‹µ í˜•ì‹ ë¬¸ì œì¸ì§€ ë°”ë¡œ ê°ˆë¼ì§‘ë‹ˆë‹¤."
)
