import re
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
import streamlit as st

from bs4 import BeautifulSoup
import tldextract
import plotly.express as px


# -----------------------------
# Page Config
# -----------------------------
st.set_page_config(
    page_title="ì„¹ì…˜ë³„ Top 5 + ì„±í–¥ ë¶„í¬ (ì§ì „ 24ì‹œê°„)",
    page_icon="ğŸ—ï¸",
    layout="wide",
)

st.markdown(
    """
<style>
.block-container { padding-top: 1.1rem; padding-bottom: 2rem; }
small.muted { color: rgba(49,51,63,.65); }
.card {
  border: 1px solid rgba(49,51,63,.15);
  border-radius: 14px;
  padding: 14px 16px;
  margin-bottom: 10px;
  background: rgba(255,255,255,.02);
}
.card h4 { margin: 0 0 8px 0; }
.kv { display: flex; gap: 12px; flex-wrap: wrap; margin: 6px 0 10px 0; }
.kv span { font-size: 0.92rem; color: rgba(49,51,63,.72); }
.badge { display:inline-block; padding:2px 8px; border-radius: 999px; border:1px solid rgba(49,51,63,.18); font-size:.82rem;}
hr.soft { border: none; border-top: 1px solid rgba(49,51,63,.10); margin: 14px 0; }
ul.tight { margin: 0.2rem 0 0.2rem 1.2rem; }
</style>
""",
    unsafe_allow_html=True,
)

# -----------------------------
# Constants
# -----------------------------
KST = timezone(timedelta(hours=9))
UTC = timezone.utc

GDELT_DOC_ENDPOINT = "https://api.gdeltproject.org/api/v2/doc/doc"
USER_AGENT = "Mozilla/5.0 (compatible; StreamlitSectionTop5/2.2; +https://streamlit.io)"
REQUEST_TIMEOUT = 10  # seconds

BIAS_ORDER = ["ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´", "ë¯¸ë¶„ë¥˜"]


# -----------------------------
# Models / Config
# -----------------------------
@dataclass(frozen=True)
class SectionQuery:
    section: str
    domestic_query: str
    overseas_query: str


SECTIONS: List[SectionQuery] = [
    SectionQuery(
        section="ì •ì¹˜",
        domestic_query='(ì •ì¹˜ OR ì •ë¶€ OR êµ­íšŒ OR ëŒ€í†µë ¹ OR ì—¬ë‹¹ OR ì•¼ë‹¹ OR ì´ì„  OR ëŒ€ì„  OR ì„ ê±° OR ê³µì²œ)',
        overseas_query='(politics OR government OR parliament OR congress OR president OR election OR campaign)',
    ),
    SectionQuery(
        section="ê²½ì œ",
        domestic_query='(ê²½ì œ OR ì¦ì‹œ OR ì£¼ì‹ OR ì½”ìŠ¤í”¼ OR ì½”ìŠ¤ë‹¥ OR í™˜ìœ¨ OR ê¸ˆë¦¬ OR ë¬¼ê°€ OR ì¸í”Œë ˆì´ì…˜ OR ê¸°ì—… OR ì‚°ì—… OR ë°˜ë„ì²´)',
        overseas_query='(economy OR markets OR stocks OR inflation OR interest rates OR central bank OR currency OR business OR industry OR semiconductor)',
    ),
    SectionQuery(
        section="ì‚¬íšŒ",
        domestic_query='(ì‚¬íšŒ OR ì‚¬ê±´ OR ì‚¬ê³  OR ë²”ì£„ OR ì¬ë‚œ OR êµìœ¡ OR ì˜ë£Œ OR ë³µì§€ OR ë…¸ë™ OR íŒŒì—… OR ë²•ì› OR ê²€ì°° OR ê²½ì°°)',
        overseas_query='(society OR crime OR accident OR disaster OR education OR health OR welfare OR labor OR strike OR court OR police)',
    ),
    SectionQuery(
        section="êµ­ì œ",
        domestic_query='(êµ­ì œ OR ì™¸êµ OR ì •ìƒíšŒë‹´ OR UN OR ìœ ì—” OR ë¯¸êµ­ OR ì¤‘êµ­ OR ì¼ë³¸ OR ëŸ¬ì‹œì•„ OR ìš°í¬ë¼ì´ë‚˜ OR ì¤‘ë™ OR ê°€ì)',
        overseas_query='(world OR international OR diplomacy OR summit OR UN OR Ukraine OR Russia OR China OR Japan OR Middle East OR Gaza)',
    ),
    SectionQuery(
        section="ìŠ¤í¬ì¸ ",
        domestic_query='(ìŠ¤í¬ì¸  OR ì¶•êµ¬ OR ì•¼êµ¬ OR ë†êµ¬ OR ë°°êµ¬ OR ê³¨í”„ OR eìŠ¤í¬ì¸  OR ì˜¬ë¦¼í”½ OR ì›”ë“œì»µ OR KBO OR Kë¦¬ê·¸)',
        overseas_query='(sports OR football OR soccer OR baseball OR basketball OR Olympics OR World Cup OR NBA OR MLB OR NHL)',
    ),
]


# -----------------------------
# Bias mapping (starter; user-editable)
# -----------------------------
def default_bias_mapping_df() -> pd.DataFrame:
    data = [
        # Korea (illustrative)
        ("chosun.com", "ë³´ìˆ˜"),
        ("donga.com", "ë³´ìˆ˜"),
        ("joongang.co.kr", "ì¤‘ë„"),
        ("mk.co.kr", "ì¤‘ë„"),
        ("yonhapnews.co.kr", "ì¤‘ë„"),
        ("hani.co.kr", "ì§„ë³´"),
        ("khan.co.kr", "ì§„ë³´"),
        # Global (illustrative)
        ("reuters.com", "ì¤‘ë„"),
        ("apnews.com", "ì¤‘ë„"),
        ("bbc.co.uk", "ì¤‘ë„"),
        ("economist.com", "ì¤‘ë„"),
        ("foxnews.com", "ë³´ìˆ˜"),
        ("wsj.com", "ë³´ìˆ˜"),
        ("nytimes.com", "ì§„ë³´"),
        ("washingtonpost.com", "ì§„ë³´"),
        ("cnn.com", "ì§„ë³´"),
    ]
    return pd.DataFrame(data, columns=["domain", "bias"])


# -----------------------------
# Helpers: text / parsing
# -----------------------------
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()


def escape_html(s: str) -> str:
    s = s or ""
    return (
        s.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#39;")
    )


def normalize_domain(url: str) -> Optional[str]:
    if not url or not isinstance(url, str):
        return None
    try:
        ext = tldextract.extract(url)
        return ext.registered_domain.lower() if ext.registered_domain else None
    except Exception:
        return None


def parse_seendate_utc(s: str) -> Optional[datetime]:
    if not s or not isinstance(s, str):
        return None
    try:
        return datetime.strptime(s, "%Y%m%d%H%M%S").replace(tzinfo=UTC)
    except Exception:
        return None


def rolling_24h_range_utc() -> Tuple[datetime, datetime]:
    """
    ê²€ìƒ‰ ì‹¤í–‰ ì‹œì  ê¸°ì¤€ ì§ì „ 24ì‹œê°„ ë²”ìœ„(UTC).
    """
    end_utc = datetime.now(UTC)
    start_utc = end_utc - timedelta(hours=24)
    return start_utc, end_utc


def build_section_query(region: str, section_cfg: SectionQuery, extra_keyword: str) -> str:
    """
    êµ­ë‚´: language:kor + ì„¹ì…˜ í‚¤ì›Œë“œ (+ optional extra keyword)
    í•´ì™¸: language:eng -sourceCountry:KOR + ì„¹ì…˜ í‚¤ì›Œë“œ (+ optional extra keyword)
    """
    extra = clean_text(extra_keyword)
    extra_part = f'("{extra}")' if extra else ""

    if region == "êµ­ë‚´":
        base = "language:kor"
        sec = section_cfg.domestic_query
        return f"{base} {sec} {extra_part}".strip()

    base = "language:eng -sourceCountry:KOR"
    sec = section_cfg.overseas_query
    return f"{base} {sec} {extra_part}".strip()


# -----------------------------
# GDELT fetch
# -----------------------------
@st.cache_data(ttl=60 * 10, show_spinner=False)
def fetch_gdelt_articles(
    query: str,
    start_dt_utc: datetime,
    end_dt_utc: datetime,
    max_records: int,
) -> pd.DataFrame:
    def fmt(dt: datetime) -> str:
        return dt.astimezone(UTC).strftime("%Y%m%d%H%M%S")

    params = {
        "query": query,
        "mode": "ArtList",
        "format": "json",
        "maxrecords": int(max_records),
        "sort": "HybridRel",
        "startdatetime": fmt(start_dt_utc),
        "enddatetime": fmt(end_dt_utc),
    }

    headers = {"User-Agent": USER_AGENT}
    r = requests.get(GDELT_DOC_ENDPOINT, params=params, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()
    data = r.json()

    rows = []
    for a in (data.get("articles", []) or []):
        url = a.get("url")
        rows.append(
            {
                "title": clean_text(a.get("title") or ""),
                "url": url,
                "seendate": a.get("seendate"),
                "published_utc": parse_seendate_utc(a.get("seendate")),
                "sourceCountry": a.get("sourceCountry"),
                "language": a.get("language"),
                "domain": normalize_domain(url) or "unknown",
            }
        )

    if not rows:
        return pd.DataFrame(columns=["title", "url", "seendate", "published_utc", "sourceCountry", "language", "domain"])

    df = pd.DataFrame(rows)
    df["published_utc"] = pd.to_datetime(df["published_utc"], utc=True, errors="coerce")
    df = df.dropna(subset=["published_utc"])
    df = df[df["title"] != ""]
    df = df.drop_duplicates(subset=["url"], keep="first")
    return df


# -----------------------------
# Summarization: 3 bullets + fallback meta
# -----------------------------
@st.cache_data(ttl=60 * 60, show_spinner=False)
def fetch_page_text_and_meta(url: str) -> Tuple[str, str]:
    if not url:
        return "", ""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }
    try:
        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        if r.status_code >= 400:
            return "", ""
        soup = BeautifulSoup(r.text, "lxml")

        desc = ""
        og = soup.find("meta", property="og:description")
        if og and og.get("content"):
            desc = clean_text(og.get("content"))

        if not desc:
            meta = soup.find("meta", attrs={"name": "description"})
            if meta and meta.get("content"):
                desc = clean_text(meta.get("content"))

        paras = soup.find_all("p")
        texts = []
        for p in paras[:10]:
            t = clean_text(p.get_text(" ", strip=True))
            if len(t) >= 40:
                texts.append(t)

        body = " ".join(texts)
        body = body[:2000]
        return body, desc
    except Exception:
        return "", ""


def split_sentences(text: str) -> List[str]:
    text = clean_text(text)
    if not text:
        return []
    parts = re.split(r"(?<=[\.\!\?])\s+|(?<=\n)\s*", text)
    out: List[str] = []
    for p in parts:
        p = clean_text(p)
        if 25 <= len(p) <= 220:
            out.append(p)

    seen = set()
    uniq = []
    for s in out:
        key = re.sub(r"[^0-9A-Za-zê°€-í£]+", "", s).lower()
        if key and key not in seen:
            seen.add(key)
            uniq.append(s)
    return uniq


def summarize_3_bullets(page_text: str, meta_desc: str) -> List[str]:
    sents = split_sentences(page_text)
    bullets: List[str] = []

    for s in sents:
        if len(bullets) >= 3:
            break
        low = s.lower()
        if any(k in low for k in ["cookies", "subscribe", "sign up", "ê´‘ê³ ", "ì €ì‘ê¶Œ", "ë¬´ë‹¨", "êµ¬ë…"]):
            continue
        bullets.append(s)

    if len(bullets) < 3 and meta_desc:
        md = clean_text(meta_desc)
        chunks = re.split(r"[â€¢\-\|/]\s*", md)
        for c in chunks:
            c = clean_text(c)
            if 25 <= len(c) <= 220 and c not in bullets:
                bullets.append(c)
            if len(bullets) >= 3:
                break

    return bullets[:3]


# -----------------------------
# Dedup clustering (token Jaccard) - SAFE
# -----------------------------
STOPWORDS_KO = set(
    "ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ë˜í•œ ë•Œë¬¸ì— í†µí•´ ê´€ë ¨ ëŒ€í•œ ë”°ë¥´ë©´ ê²½ìš° ì´ë²ˆ ì˜¤ëŠ˜ ë‚´ì¼ ì–´ì œ ê¸°ì ë‹¨ë… ì†ë³´ "
    "ì˜ìƒ ì‚¬ì§„ ë°œí‘œ ë°í˜”ë‹¤ ë§í–ˆë‹¤ ì˜ˆì • ì§„í–‰ ê°€ëŠ¥ í™•ëŒ€ ê°ì†Œ ì¦ê°€ ì •ë¶€ êµ­íšŒ ëŒ€í†µë ¹ "
    .split()
)
STOPWORDS_EN = set(
    "the a an and or but if then than this that those these to of in on for with without "
    "as from by at is are was were be been being it its into about after before over under "
    "says said say will would could should may might "
    .split()
)


def title_tokens(title: str) -> List[str]:
    t = clean_text(title).lower()
    t = re.sub(r"https?://\S+", " ", t)
    t = re.sub(r"[^0-9a-zê°€-í£\s]", " ", t)
    toks = [x for x in t.split() if len(x) >= 2]
    filtered: List[str] = []
    for x in toks:
        if re.fullmatch(r"\d+", x):
            filtered.append(x)
            continue
        if x in STOPWORDS_EN or x in STOPWORDS_KO:
            continue
        filtered.append(x)
    return filtered[:32]


def jaccard(a: set, b: set) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0


def dedup_by_title_cluster(df: pd.DataFrame, sim_threshold: float = 0.62) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if "title" not in df.columns or "published_utc" not in df.columns:
        return df

    dfx = df.copy().sort_values("published_utc", ascending=False)

    kept_idx: List[int] = []
    cluster_reps: List[set] = []

    for idx, row in dfx.iterrows():
        toks = set(title_tokens(row.get("title", "")))
        if not toks:
            continue

        dup = False
        for rep in cluster_reps:
            if jaccard(toks, rep) >= sim_threshold:
                dup = True
                break

        if not dup:
            kept_idx.append(idx)
            cluster_reps.append(toks)

    if not kept_idx:
        return dfx.head(0)

    return dfx.loc[kept_idx].copy()


# -----------------------------
# Bias mapping apply + distribution
# -----------------------------
def apply_bias_mapping(df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    out = df.copy()
    out["bias"] = out["domain"].map(lambda d: mapping.get((d or "").lower(), "ë¯¸ë¶„ë¥˜"))
    out["bias"] = out["bias"].where(out["bias"].isin(["ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´"]), "ë¯¸ë¶„ë¥˜")
    return out


def distribution(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty or "bias" not in df.columns:
        return pd.DataFrame(columns=["bias", "count", "share"])
    dist = df.groupby("bias").size().reset_index(name="count")
    total = dist["count"].sum()
    dist["share"] = dist["count"] / total if total else 0
    dist["bias"] = pd.Categorical(dist["bias"], categories=BIAS_ORDER, ordered=True)
    return dist.sort_values("bias")


# -----------------------------
# Rendering
# -----------------------------
def render_top_list(section_name: str, top_df: pd.DataFrame, enable_summary: bool):
    st.subheader(f"{section_name} Â· Top {len(top_df)}")
    if top_df is None or top_df.empty:
        st.warning("í•´ë‹¹ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ê¸°ê°„/í‚¤ì›Œë“œ/ë²”ìœ„ ì¡°ì • í•„ìš”)")
        return

    for idx, row in top_df.reset_index(drop=True).iterrows():
        title = row.get("title") or "(ì œëª© ì—†ìŒ)"
        url = row.get("url") or ""
        domain = row.get("domain") or "unknown"
        bias = row.get("bias") or "ë¯¸ë¶„ë¥˜"

        pub_str = ""
        try:
            pub_kst = pd.to_datetime(row.get("published_utc"), utc=True).tz_convert(KST)
            pub_str = pub_kst.strftime("%Y-%m-%d %H:%M (KST)")
        except Exception:
            pass

        bullets: List[str] = row.get("bullets") or []
        meta_desc = row.get("meta_desc") or ""

        if enable_summary:
            if bullets:
                summary_html = "<ul class='tight'>" + "".join([f"<li>{escape_html(b)}</li>" for b in bullets]) + "</ul>"
            elif meta_desc:
                summary_html = f"<small class='muted'>{escape_html(meta_desc)}</small>"
            else:
                summary_html = "<small class='muted'>ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ì‚¬ì´íŠ¸ ì°¨ë‹¨/ë©”íƒ€ì •ë³´/ë³¸ë¬¸ ë¶€ì¬ ê°€ëŠ¥).</small>"
        else:
            summary_html = "<small class='muted'>ìš”ì•½ ê¸°ëŠ¥ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤.</small>"

        st.markdown(
            f"""
<div class="card">
  <div class="kv">
    <span class="badge">#{idx+1}</span>
    <span>ì„±í–¥: <b>{escape_html(bias)}</b></span>
    <span>ë„ë©”ì¸: <b>{escape_html(domain)}</b></span>
    <span>ë°œí–‰: <b>{escape_html(pub_str)}</b></span>
  </div>
  <h4>{escape_html(title)}</h4>
  <div>
    <a href="{url}" target="_blank" rel="noopener noreferrer">{url}</a>
  </div>
  <hr class="soft"/>
  <div>
    <b>í•µì‹¬ ìš”ì•½ (3 bullets)</b><br/>
    {summary_html}
  </div>
</div>
""",
            unsafe_allow_html=True,
        )


# -----------------------------
# UI
# -----------------------------
st.title("ì„¹ì…˜ë³„ ì£¼ìš” ë‰´ìŠ¤ Top 5 + ì„±í–¥ ë¶„í¬")
st.caption("êµ­ë‚´/í•´ì™¸ ì„ íƒ í›„ ì„¹ì…˜ë³„ Top 5ë¥¼ â€˜ì¤‘ë³µ ì œê±° + 3ì¤„ ìš”ì•½â€™ìœ¼ë¡œ ê°œì„ í•˜ê³ , ì„¹ì…˜ë³„ ì„±í–¥ ë¶„í¬ë¥¼ í•¨ê»˜ ë³´ì—¬ì¤ë‹ˆë‹¤. (ë°ì´í„°: GDELT)")

with st.sidebar:
    st.header("1) ë²”ìœ„ ì„ íƒ")
    region = st.radio("êµ­ë‚´/í•´ì™¸", options=["êµ­ë‚´", "í•´ì™¸"], horizontal=True)

    st.divider()
    st.header("2) ì„¹ì…˜ ì„ íƒ")
    section_names = [s.section for s in SECTIONS]
    selected_sections = st.multiselect(
        "ë¶„ì„í•  ì„¹ì…˜(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
        options=section_names,
        default=section_names,
    )

    st.divider()
    st.header("3) Top ë‰´ìŠ¤ êµ¬ì„±")
    extra_keyword = st.text_input(
        "ì¶”ê°€ í‚¤ì›Œë“œ(ì„ íƒ)",
        value="",
        help="ì˜ˆ: â€˜íƒ„ì†Œì„¸â€™, â€˜ì² ê°•â€™, â€˜ì›ì „â€™ ë“±ì„ ë„£ìœ¼ë©´ í•´ë‹¹ ì´ìŠˆ ì¤‘ì‹¬ìœ¼ë¡œ ì„¹ì…˜ë³„ Top 5ê°€ êµ¬ì„±ë©ë‹ˆë‹¤.",
    )

    top_n = st.number_input("ì„¹ì…˜ë³„ Top N", min_value=3, max_value=10, value=5, step=1)

    candidate_pool = st.number_input(
        "ì„¹ì…˜ë³„ í›„ë³´ ê¸°ì‚¬ ìˆ˜(ìˆ˜ì§‘ëŸ‰)",
        min_value=60,
        max_value=500,
        value=220,
        step=10,
        help="ê° ì„¹ì…˜ì—ì„œ Top Nì„ ë½‘ê¸° ì „ GDELTì—ì„œ ê°€ì ¸ì˜¤ëŠ” í›„ë³´ ê¸°ì‚¬ ìˆ˜ì…ë‹ˆë‹¤.",
    )

    st.divider()
    st.header("í’ˆì§ˆ ì˜µì…˜")
    enable_summary = st.toggle("3ì¤„ í•µì‹¬ bullet ìš”ì•½", value=True)
    sim_threshold = st.slider(
        "ì¤‘ë³µ ì œê±° ìœ ì‚¬ë„ ì„ê³„ê°’(Jaccard)",
        min_value=0.45,
        max_value=0.80,
        value=0.62,
        step=0.01,
        help="ê°’ì´ ë†’ì„ìˆ˜ë¡ â€˜ê±°ì˜ ê°™ì€ ì œëª©â€™ë§Œ ì¤‘ë³µìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤. 0.60~0.70 ê¶Œì¥.",
    )

    st.divider()
    st.header("ì„±í–¥ ë§¤í•‘")
    uploaded = st.file_uploader("ë§¤í•‘ CSV ì—…ë¡œë“œ (domain,bias)", type=["csv"])
    if uploaded is not None:
        try:
            map_df = pd.read_csv(uploaded)[["domain", "bias"]].dropna()
        except Exception:
            st.warning("CSVë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. columns: domain,bias í˜•íƒœì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            map_df = default_bias_mapping_df()
    else:
        map_df = default_bias_mapping_df()

    edited_map_df = st.data_editor(
        map_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "domain": st.column_config.TextColumn("domain"),
            "bias": st.column_config.SelectboxColumn("bias", options=["ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´"]),
        },
    )

    mapping_dict = {
        str(r["domain"]).strip().lower(): str(r["bias"]).strip()
        for _, r in edited_map_df.dropna().iterrows()
        if str(r.get("domain", "")).strip() and str(r.get("bias", "")).strip()
    }

    st.divider()
    debug = st.toggle("ë””ë²„ê·¸ í‘œì‹œ(ì¿¼ë¦¬/ê±´ìˆ˜)", value=False)

    run = st.button("ì§ì „ 24ì‹œê°„ ì„¹ì…˜ë³„ Top ë‰´ìŠ¤ ìƒì„±", type="primary", use_container_width=True)

if not run:
    st.info("ì¢Œì¸¡ì—ì„œ ë²”ìœ„/ì„¹ì…˜/ì˜µì…˜ì„ ì„ íƒí•œ ë’¤ ì‹¤í–‰í•˜ì„¸ìš”.")
    st.stop()

if not selected_sections:
    st.warning("ìµœì†Œ 1ê°œ ì„¹ì…˜ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    st.stop()

start_utc, end_utc = rolling_24h_range_utc()
start_kst = start_utc.astimezone(KST)
end_kst = end_utc.astimezone(KST)

st.markdown(f"### {region} Â· ì„¹ì…˜ë³„ Top {int(top_n)}")
st.caption(f"ìˆ˜ì§‘ ê¸°ê°„: {start_kst.strftime('%Y-%m-%d %H:%M')} ~ {end_kst.strftime('%Y-%m-%d %H:%M')} (KST, ì§ì „ 24ì‹œê°„)")

section_cfg_map: Dict[str, SectionQuery] = {s.section: s for s in SECTIONS}
results: Dict[str, Dict[str, pd.DataFrame]] = {}

with st.spinner("ì„¹ì…˜ë³„ ê¸°ì‚¬ í›„ë³´ë¥¼ ìˆ˜ì§‘/ì •ì œ ì¤‘ì…ë‹ˆë‹¤..."):
    for sec_name in selected_sections:
        cfg = section_cfg_map[sec_name]
        q = build_section_query(region, cfg, extra_keyword)

        try:
            df = fetch_gdelt_articles(
                query=q,
                start_dt_utc=start_utc,
                end_dt_utc=end_utc,
                max_records=int(candidate_pool),
            )
        except Exception:
            df = pd.DataFrame(columns=["title", "url", "seendate", "published_utc", "sourceCountry", "language", "domain"])

        if debug:
            st.write(f"[DEBUG] {sec_name} query = {q}")
            st.write(f"[DEBUG] {sec_name} fetched rows = {len(df)}")

        # ì„±í–¥ ë§¤í•‘
        df = apply_bias_mapping(df, mapping_dict)

        # ì¤‘ë³µ ì œê±°(ì»¬ëŸ¼ ë³´ì¡´ ì•ˆì „)
        df_dedup = dedup_by_title_cluster(df, sim_threshold=float(sim_threshold))

        # ë°©ì–´: published_utc ì—†ìœ¼ë©´ ë¹ˆ DFë¡œ
        if df_dedup is None or "published_utc" not in df_dedup.columns:
            df_dedup = df.head(0).copy()

        if not df_dedup.empty:
            df_dedup = df_dedup.sort_values("published_utc", ascending=False)

        top_df = df_dedup.head(int(top_n)).copy()

        # 3ì¤„ ìš”ì•½
        if enable_summary and not top_df.empty:
            top_df["bullets"] = None
            top_df["meta_desc"] = ""
            for i in range(len(top_df)):
                url = top_df.iloc[i].get("url")
                time.sleep(0.12)
                page_text, meta_desc = fetch_page_text_and_meta(url)
                bullets = summarize_3_bullets(page_text, meta_desc)
                top_df.iat[i, top_df.columns.get_loc("bullets")] = bullets
                top_df.iat[i, top_df.columns.get_loc("meta_desc")] = meta_desc or ""

        dist_df = distribution(df_dedup)

        results[sec_name] = {
            "candidates": df_dedup,
            "top": top_df,
            "dist": dist_df,
            "query": pd.DataFrame([{"query": q}]),
        }

# Render tabs
tabs = st.tabs(selected_sections)
for tab, sec_name in zip(tabs, selected_sections):
    with tab:
        q = results[sec_name]["query"].iloc[0]["query"]
        st.markdown(f"<small class='muted'>ì‚¬ìš© ì¿¼ë¦¬: {escape_html(clean_text(q))}</small>", unsafe_allow_html=True)

        cands = results[sec_name]["candidates"]
        dist_df = results[sec_name]["dist"]

        c1, c2, c3 = st.columns(3)
        c1.metric("í›„ë³´ ê¸°ì‚¬(ì¤‘ë³µ ì œê±° í›„)", f"{len(cands):,}" if cands is not None else "0")
        unknown_share = dist_df.loc[dist_df["bias"] == "ë¯¸ë¶„ë¥˜", "share"].sum() if dist_df is not None and not dist_df.empty else 0
        c2.metric("ë¯¸ë¶„ë¥˜ ë¹„ìœ¨", f"{unknown_share*100:.1f}%")
        c3.metric("ê³ ìœ  ë„ë©”ì¸", f"{cands['domain'].nunique(dropna=True):,}" if cands is not None and not cands.empty and "domain" in cands.columns else "0")

        if dist_df is not None and not dist_df.empty:
            fig = px.bar(dist_df, x="bias", y="count", text=dist_df["share"].map(lambda x: f"{x*100:.1f}%"))
            fig.update_layout(xaxis_title="ì„±í–¥", yaxis_title="ê¸°ì‚¬ ìˆ˜", showlegend=False, height=320)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ì„±í–¥ ë¶„í¬ë¥¼ ë§Œë“¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        st.divider()
        render_top_list(sec_name, results[sec_name]["top"], enable_summary)

        with st.expander("ì§„ë‹¨: í›„ë³´ ê¸°ì‚¬(ì¤‘ë³µ ì œê±° í›„) ë¯¸ë¦¬ë³´ê¸°", expanded=False):
            if cands is None or cands.empty:
                st.write("í›„ë³´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                cols = [c for c in ["published_utc", "bias", "domain", "title", "url", "language", "sourceCountry"] if c in cands.columns]
                st.dataframe(cands[cols].head(60), use_container_width=True, height=420)

st.caption(
    "ì£¼ì˜: (1) ì„¹ì…˜ ë¶„ë¥˜ëŠ” ì„¹ì…˜ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ê¸°ë°˜ì´ë©°, (2) ìš”ì•½ì€ ì›¹í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥ ë²”ìœ„ì—ì„œë§Œ ìƒì„±ë©ë‹ˆë‹¤. "
    "ì •í™•ë„ë¥¼ ë” ë†’ì´ë ¤ë©´ â€˜ì–¸ë¡ ì‚¬ë³„ RSS/ì„¹ì…˜ URLâ€™ ê¸°ë°˜ ìˆ˜ì§‘ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
)
